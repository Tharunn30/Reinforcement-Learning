{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYclSKTPr/LF9/ETS20oEB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"W01G64KcFR8T","executionInfo":{"status":"error","timestamp":1709784390279,"user_tz":-330,"elapsed":392,"user":{"displayName":"P.V. THARUNN RAJ (RA2112701010004)","userId":"17131910621466126646"}},"outputId":"054228ca-9d8f-434a-e1cf-9304138b6263"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'policy_iteration' object has no attribute 'val_iteration'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-eb8640639782>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'policy_iteration' object has no attribute 'val_iteration'"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","p_heads = .4\n","GAMMA = 1\n","rewards = np.zeros(101) #rewards for each state including 100\n","rewards[100] = 1\n","\n","class policy_iteration:\n","\tdef __init__(self):\n","\t\tself.val_state = np.zeros(101)\n","\t\tself.policy = np.zeros(100)\n","\n","\tdef bellman(self, state, action, val_state):\n","\t\treturn p_heads * (rewards[state + action] + GAMMA * val_state[state + action]) + (1 - p_heads) * (rewards[state - action] + GAMMA * val_state[state - action])\n","\n","\tdef policy_evaluation(self, epsilon = 0.00000000000001):\n","\t\twhile True:\n","\t\t\tdelta = 0\n","\t\t\t#print('STARTING POLICY EVALUATION')\n","\t\t\tfor state in range(1,100):\n","\t\t\t\tv = self.val_state[state]\n","\t\t\t\t# print('state', state)\n","\t\t\t\t# print('policy at state', self.policy[state])\n","\t\t\t\t# print('val state', self.val_state)\n","\t\t\t\tself.val_state[state] = self.bellman(state, int(self.policy[state]), self.val_state)\n","\t\t\t\t#val_state[state] is weighted sum over all possible transitions for the policy from this state\n","\t\t\t\tdelta = max(delta, np.abs(self.val_state[state] - v))\n","\t\t\tif delta < epsilon:\n","\t\t\t\tbreak\n","\t\treturn self.policy_improvement()\n","\n","\tdef policy_improvement(self):\n","\t\tpolicy_stable = True\n","\t\t#print('STARTING POLICY IMPROVEMENT')\n","\t\tfor state in range(1,100):\n","\t\t\told_action = self.policy[state]\n","\t\t\tmax_action = min(state, 100-state)\n","\t\t\tval_action = np.zeros(max_action+1)\n","\t\t\tfor action in range(1, max_action+1):\n","\t\t\t\tval_action[action] = self.bellman(state, action, self.val_state)\n","\t\t\t#x = np.argwhere(val_action == np.max(val_action))\n","\t\t\t#self.policy[state] = x[-1]\n","\t\t\tself.policy[state] = np.argmax(val_action)\n","\t\t\t# print('state', state)\n","\t\t\t# print(np.argwhere(val_action >= np.max(val_action)).squeeze())\n","\t\t\t# print(self.policy[state])\n","\t\t\t# #print(np.argwhere(val_action >= np.max(val_action)*0.99).squeeze())\n","\t\t\t# print(np.random.choice(np.argwhere(val_action == np.max(val_action)).squeeze()))\n","\t\t\t#self.policy[state] = np.random.choice(np.argwhere(val_action == np.max(val_action)).squeeze())\n","\n","\t\t\tif old_action != self.policy[state]:\n","\t\t\t\tpolicy_stable = False\n","\t\t\t\tprint('Not stable state', state)\n","\t\tprint('Stable situation', policy_stable)\n","\t\tif policy_stable != True:\n","\t\t\t\tself.policy_evaluation()\n","\t\treturn self.val_state, self.policy\n","\n","vi = policy_iteration()\n","v, p = vi.val_iteration()\n","print(v)\n","print(p)\n","\n","# pi = policy_iteration()\n","# v, p = pi.policy_evaluation()\n","# print(v)\n","# print(p)\n","\n","plt.plot(range(100), v[:100])\n","plt.xlabel('Capital')\n","plt.ylabel('Value estimates')\n","plt.show()\n","\n","plt.bar(range(100), p, align = 'center', alpha = 0.5)\n","for i in range(100):\n","  plt.text(i - 0.75, p[i] + 0.01, str(round(v[i],2)), fontsize=6)\n","plt.xlabel('Capital')\n","plt.xticks(np.arange(0, 101, 5))\n","plt.ylabel('Final policy (stake)')\n","plt.show()\n","\n","\n"]}]}